# Elect_Eval Dataset

Welcome to the Elect_Eval dataset repository! This dataset is crafted to assist in testing and evaluating AI models, specifically GPT-3.5 Turbo and LLaMA 3 8B - Instruct, in generating and assessing responses to open-ended exam or quiz questions. Below, you'll find comprehensive information on the dataset structure, preparation, and test results.

## Dataset Overview

Elect_Eval consists of two subsets for different models:

- **GPT-3.5 Turbo**
- **LLaMA 3**

Each subset contains:

- **User Input Data:** User-provided instructions and contexts.
- **Additional Context:** Supplementary information to enhance model responses.
- **Model Outputs:** Responses generated by each model.
- **Expected Output:** Reference answers for performance evaluation.

The dataset is sourced from seven exam papers (70 questions total) on general electricity, used in a second-year civil engineering course at ULPGL Goma.

## Dataset Structure

The dataset is comprised of four columns:

- **answer:** JSON format model-generated corrections, including overall comments, total scores, and specific feedback for each question.
- **question:** The combination of exam content and student responses in text format.
- **context:** Contextual information providing a reference framework for the model.
- **ground_truth:** Manual corrections by the teacher in JSON format, similar to model-generated responses.

## Data Preparation

Original handwritten exam papers were transcribed into digital format. The initial papers lacked feedback, so GPT-4o was employed to generate feedbacks and comments. GPT-4o is known for high performance on standardized benchmarks, scoring 88.7% in 5-shot mode on the MMLU benchmark.

## Test Results

Tests were conducted using the Azure AI Studio's automatic evaluation tools. Below are the results from different datasets:

## Manual evaluation

![GPT-Test-Result](/imgs/Resultat-GPT-3-Turbo.PNG)

### Datasets-1

1. **elec_Eval_GPT25-Turbo:**

   - Automatic Test Result:
     - ![GPT-Test-Result](/imgs/evaluation_eval-gpt35_tubo-2.png)

2. **elec_Eval_Llama_3:**
   - Test Results:
     - ![Llama-Test-Result](/imgs/evaluation_eval-Llama3.png)

Despite misalignments, GPT3.5 Turbo showed closer alignment with the teacher's grading.

### Datasets-2

Updates were made by tuning prompts and modifying data in the ground_truth column. Only GPT3.5 Turbo was tested:

- **Improved Results:**
  - ![Improved-Result](/imgs/evaluation_Eval_elecGPT3_Turb-improved.png)

Improvements are noted over previous evaluations.

## Conclusion

This Elect_Eval dataset allows for an in-depth analysis of model efficiency in educational grading scenarios. Our improvements are aimed at enhancing alignment with human grading, though continued refinement is needed.

We welcome feedback and contributions to further develop this resource.

For any inquiries or contributions, please reach out: ashuzamh@gami.com

---

Thank you for exploring the Elect_Eval dataset!
